{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2loUE1UlfjId"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"/home/mega/Desktop/Data2/Datasets/Thyroid_Dataset/Five_Cats_Without_Preprocessing_Manual_Adding_Images_Folder_Whole_Slide.zip\",\"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"/home/mega/Desktop/Data2/Datasets/Thyroid_Dataset/Five_Cats_Without_Preprocessing_Manual_Adding_Images_Folder_Whole_Slide\")\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOLM0BnhfjIh"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"/home/mega/Desktop/Data2/Datasets/Thyroid_Dataset/pytorch_dataset_duc.zip\",\"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"/home/mega/Desktop/Data2/Datasets/Thyroid_Dataset/pytorch_dataset_duc\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7cLbNsYfjIi",
        "outputId": "3f25de1a-b61f-4819-ed6c-a73bf8baa9b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CC Moudle\n",
            "Loaded pretrained weights for efficientnet-b1\n",
            "Loaded pretrained weights for efficientnet-b0\n",
            "Loaded pretrained weights for efficientnet-b3\n",
            "Loaded pretrained weights for efficientnet-b5\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import shutil\n",
        "from sklearn import model_selection\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import plot_precision_recall_curve\n",
        "from sklearn.metrics import average_precision_score\n",
        "from CSRNet_VGG_Criss_Cross import *\n",
        "#from Efficient_Net import EfficientNet_b0\n",
        "from Ensemble_Learning import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAZqKkfvfjIk"
      },
      "outputs": [],
      "source": [
        "WD = '/home/mega/Desktop/Data2/Datasets/Thyroid_Dataset/dataset_NormReinhard_5_Classes/dataset_NormReinhard_5_Classes/'\n",
        "Cat2_Folder = WD + 'category_2_NormReinhard_bg_crops_dataset'\n",
        "Cat3_Folder = WD + 'category_3_NormReinhard_bg_crops_dataset'\n",
        "Cat4_Folder = WD + 'category_4_NormReinhard_bg_crops_dataset'\n",
        "Cat5_Folder = WD + 'category_5_NormReinhard_bg_crops_dataset'\n",
        "Cat6_Folder = WD + 'category_6_NormReinhard_bg_crops_dataset'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXbOlD_SfjIl"
      },
      "outputs": [],
      "source": [
        "cat2_images = os.listdir(Cat2_Folder)\n",
        "cat3_images = os.listdir(Cat3_Folder)\n",
        "cat4_images = os.listdir(Cat4_Folder)\n",
        "cat5_images = os.listdir(Cat5_Folder)\n",
        "cat6_images = os.listdir(Cat6_Folder)\n",
        "cat2_images[:4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIdXmsy7fjIl"
      },
      "outputs": [],
      "source": [
        "len(cat2_images),len(cat3_images),len(cat4_images),len(cat5_images),len(cat6_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04Kfcy40fjIm"
      },
      "outputs": [],
      "source": [
        "plt.imshow(Image.open(os.path.join(Cat2_Folder,cat2_images[3])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UU4WjBHkfjIm"
      },
      "outputs": [],
      "source": [
        "Pytorch_Dataset = '/home/mega/Desktop/Data2/Datasets/Thyroid_Dataset/pytorch_dataset'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWkxC5AUfjIm"
      },
      "outputs": [],
      "source": [
        "# Generate folder Structure\n",
        "try:\n",
        "    os.makedirs(os.path.join(Pytorch_Dataset, 'train/cat2'))\n",
        "    os.makedirs(os.path.join(Pytorch_Dataset, 'train/cat3'))\n",
        "    os.makedirs(os.path.join(Pytorch_Dataset, 'train/cat4'))\n",
        "    os.makedirs(os.path.join(Pytorch_Dataset, 'train/cat5'))\n",
        "    os.makedirs(os.path.join(Pytorch_Dataset, 'train/cat6'))\n",
        "    \n",
        "    os.makedirs(os.path.join(Pytorch_Dataset, 'val/cat2'))\n",
        "    os.makedirs(os.path.join(Pytorch_Dataset, 'val/cat3'))\n",
        "    os.makedirs(os.path.join(Pytorch_Dataset, 'val/cat4'))\n",
        "    os.makedirs(os.path.join(Pytorch_Dataset, 'val/cat5'))\n",
        "    os.makedirs(os.path.join(Pytorch_Dataset, 'val/cat6'))\n",
        "    \n",
        "    os.makedirs(os.path.join(Pytorch_Dataset, 'test/cat2'))\n",
        "    os.makedirs(os.path.join(Pytorch_Dataset, 'test/cat3'))\n",
        "    os.makedirs(os.path.join(Pytorch_Dataset, 'test/cat4'))\n",
        "    os.makedirs(os.path.join(Pytorch_Dataset, 'test/cat5'))\n",
        "    os.makedirs(os.path.join(Pytorch_Dataset, 'test/cat6'))\n",
        "    \n",
        "except:\n",
        "    print('Already Exist')\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHOiJ9m_fjIn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XC3Tp1BofjIo"
      },
      "outputs": [],
      "source": [
        "all_images=[os.path.join(Cat2_Folder,i) for i in cat2_images] + [os.path.join(Cat3_Folder,i) for i in cat3_images] + [os.path.join(Cat4_Folder,i) for i in cat4_images] + [os.path.join(Cat5_Folder,i) for i in cat5_images] + [os.path.join(Cat6_Folder,i) for i in cat6_images]\n",
        "all_cats = ['cat2']*len(cat2_images) + ['cat3']*len(cat3_images) + ['cat4']*len(cat4_images) + ['cat5']*len(cat5_images) + ['cat6']*len(cat6_images) \n",
        "# Split and copy/move images to these folders\n",
        "# Train/Val/Test ratio 7/2/1\n",
        "x_train, x_val, y_train,y_val = model_selection.train_test_split(all_images, all_cats, test_size=3/10)\n",
        "#x_val, x_test, y_val, y_test = model_selection.train_test_split(x_val,y_val, test_size=1/3)\n",
        "len(x_train), len(x_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3zZvmw1fjIo"
      },
      "outputs": [],
      "source": [
        "for img,label in zip(x_train, y_train):\n",
        "    if '2' in label:\n",
        "        shutil.copy2(img,os.path.join(Pytorch_Dataset, 'train/cat2'))\n",
        "    elif '3' in label:\n",
        "        shutil.copy2(img,os.path.join(Pytorch_Dataset, 'train/cat3'))\n",
        "    elif '4' in label:\n",
        "        shutil.copy2(img,os.path.join(Pytorch_Dataset, 'train/cat4'))\n",
        "    elif '5' in label:\n",
        "        shutil.copy2(img,os.path.join(Pytorch_Dataset, 'train/cat5'))\n",
        "    else:\n",
        "        shutil.copy2(img,os.path.join(Pytorch_Dataset, 'train/cat6'))\n",
        "        \n",
        "for img,label in zip(x_val, y_val):\n",
        "    if '2' in label:\n",
        "        shutil.copy2(img,os.path.join(Pytorch_Dataset, 'val/cat2'))\n",
        "    elif '3' in label:\n",
        "        shutil.copy2(img,os.path.join(Pytorch_Dataset, 'val/cat3'))\n",
        "    elif '4' in label:\n",
        "        shutil.copy2(img,os.path.join(Pytorch_Dataset, 'val/cat4'))\n",
        "    elif '5' in label:\n",
        "        shutil.copy2(img,os.path.join(Pytorch_Dataset, 'val/cat5'))\n",
        "    else:\n",
        "        shutil.copy2(img,os.path.join(Pytorch_Dataset, 'val/cat6')) \n",
        "        \n",
        "# for img,label in zip(x_test, y_test):\n",
        "#     if '2' in label:\n",
        "#         shutil.copy2(img,os.path.join(Pytorch_Dataset, 'test/cat2'))\n",
        "#     elif '3' in label:\n",
        "#         shutil.copy2(img,os.path.join(Pytorch_Dataset, 'test/cat3'))\n",
        "#     elif '4' in label:\n",
        "#         shutil.copy2(img,os.path.join(Pytorch_Dataset, 'test/cat4'))\n",
        "#     elif '5' in label:\n",
        "#         shutil.copy2(img,os.path.join(Pytorch_Dataset, 'test/cat5'))\n",
        "#     else:\n",
        "#         shutil.copy2(img,os.path.join(Pytorch_Dataset, 'test/cat6'))         "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-O44cGwfjIp",
        "outputId": "04b26aaf-7ac8-4ed2-c41a-82000638b76c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Data augmentation and normalization for training\n",
        "# Just normalization for validation\n",
        "# TODO: augment with other different transformations\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        # transforms.RandomResizedCrop(),\n",
        "        transforms.RandomRotation(5),\n",
        "        transforms.RandomVerticalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "        # transforms.RandomErasing(0.05),\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "#data_dir = '/home/mega/Desktop/Data2/Datasets/Thyroid_Dataset/pytorch_dataset'\n",
        "#data_dir = '/home/mega/Desktop/Data2/Datasets/Thyroid_Dataset/pytorch_dataset_01'\n",
        "data_dir = '/home/mega/Desktop/Data2/Datasets/Thyroid_Dataset/pytorch_dataset_duc/pytorch_dataset_duc/'\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transforms[x])\n",
        "                  for x in ['train', 'val']}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=64,\n",
        "                                             shuffle=True, num_workers=10)\n",
        "              for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2H3a2GRfjIq"
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    \n",
        "    epoch_accs = []\n",
        "    epoch_losses = []\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                #labels = labels -1\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "            epoch_accs.append(epoch_acc)\n",
        "            epoch_losses.append(epoch_loss)\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    metadata = {\n",
        "        \"model\": model,\n",
        "        \"epoch_accuracy\": epoch_accs,\n",
        "        \"epoch_loss\": epoch_losses}\n",
        "    return metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOp0CEtzfjIq"
      },
      "outputs": [],
      "source": [
        "from efficientnet_pytorch import EfficientNet\n",
        "# model=EfficientNet.from_pretrained('efficientnet-b0')\n",
        "# model.extract_endpoints\n",
        "class EfficientNet_b0(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EfficientNet_b0, self).__init__()\n",
        "        self.model = EfficientNet.from_pretrained('efficientnet-b0')\n",
        "        self.CAMM = CAM_Module_01(1280)\n",
        "        self.classifier_layer = nn.Sequential(\n",
        "            nn.Linear(1280, 1200),\n",
        "            #nn.Linear(1200, 1000),\n",
        "            #nn.Linear(1000, 800),\n",
        "            #nn.Linear(800, 600),\n",
        "            #nn.Linear(600, 512),\n",
        "            #nn.BatchNorm1d(512),\n",
        "            nn.Dropout(0.6),\n",
        "            nn.Linear(1200, 512),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.Linear(256, 104),\n",
        "            nn.Linear(104, 5)\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.model.extract_features(inputs)\n",
        "        #print(x.shape)\n",
        "        x1 = self.CAMM(x)\n",
        "        x2 = self.CAMM(x)\n",
        "\n",
        "        x = x1 + x2\n",
        "        # Pooling and final linear layer\n",
        "        x = self.model._avg_pooling(x)\n",
        "        x = x.flatten(start_dim=1)\n",
        "        x = self.model._dropout(x)\n",
        "        x = self.classifier_layer(x)\n",
        "        return x\n",
        "\n",
        "class CAM_Module_01(nn.Module):\n",
        "    def __init__(self, in_dim):\n",
        "        super(CAM_Module_01, self).__init__()\n",
        "        self.chanel_in = in_dim\n",
        "\n",
        "\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "        self.softmax  = nn.Softmax(dim=-1)\n",
        "    def forward(self,x):\n",
        "        m_batchsize, C, height, width = x.size()\n",
        "        proj_query = x.view(m_batchsize, C, -1)\n",
        "        proj_key = x.view(m_batchsize, C, -1).permute(0, 2, 1)\n",
        "        energy = torch.bmm(proj_query, proj_key)\n",
        "        energy_new = torch.max(energy, -1, keepdim=True)[0].expand_as(energy)-energy\n",
        "        attention = self.softmax(energy_new)\n",
        "        proj_value = x.view(m_batchsize, C, -1)\n",
        "\n",
        "        out = torch.bmm(attention, proj_value)\n",
        "        out = out.view(m_batchsize, C, height, width)\n",
        "\n",
        "        out = self.gamma*out + x\n",
        "        return out    \n",
        "    \n",
        "# if __name__ == \"__main__\":\n",
        "#     inputs = torch.randn(1, 3, 224, 224)\n",
        "#     model = EfficientNet_b0()\n",
        "#     output = model(inputs)\n",
        "#     print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKGpeVqzfjIr",
        "outputId": "e6813380-c3c0-4566-ed1c-eff2b6b1e647"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pretrained weights for efficientnet-b0\n"
          ]
        }
      ],
      "source": [
        "# from efficientnet_pytorch import EfficientNet\n",
        "# model_ft = EfficientNet.from_pretrained('efficientnet-b0', 5)\n",
        "\n",
        "#model_ft = models.resnet50(pretrained=True)\n",
        "#num_ftrs = model_ft.fc.in_features\n",
        "# # Here the size of each output sample is set to 2.\n",
        "# # Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
        "#model_ft.fc = nn.Linear(num_ftrs, 5)\n",
        "\n",
        "#model_ft = DenseScaleNet()\n",
        "model_ft = EfficientNet_b0()\n",
        "#model_ft = Resnet50()\n",
        "#model_ft = EfficientNet(1,1)\n",
        "#model_ft = Ensemble_Net(efficient_net_b0,efficient_net_b1,resnet)\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "#optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "#optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.001)\n",
        "optimizer_ft = optim.Adam(model_ft.parameters(), lr=1e-3)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mFL64G_fjIs",
        "outputId": "e0091789-b87b-4612-aaaa-242760e1a719"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/99\n",
            "----------\n",
            "train Loss: 1.3922 Acc: 0.4114\n",
            "val Loss: 1.3945 Acc: 0.4801\n",
            "\n",
            "Epoch 1/99\n",
            "----------\n",
            "train Loss: 1.2471 Acc: 0.5125\n",
            "val Loss: 1.4828 Acc: 0.4778\n",
            "\n",
            "Epoch 2/99\n",
            "----------\n",
            "train Loss: 1.1374 Acc: 0.5593\n",
            "val Loss: 1.6797 Acc: 0.5441\n",
            "\n",
            "Epoch 3/99\n",
            "----------\n",
            "train Loss: 1.0824 Acc: 0.5870\n",
            "val Loss: 1.4056 Acc: 0.5072\n",
            "\n",
            "Epoch 4/99\n",
            "----------\n",
            "train Loss: 0.9926 Acc: 0.6235\n",
            "val Loss: 1.0137 Acc: 0.6503\n",
            "\n",
            "Epoch 5/99\n",
            "----------\n",
            "train Loss: 0.9558 Acc: 0.6466\n",
            "val Loss: 1.1771 Acc: 0.5955\n",
            "\n",
            "Epoch 6/99\n",
            "----------\n",
            "train Loss: 0.8723 Acc: 0.6693\n",
            "val Loss: 0.7556 Acc: 0.7207\n",
            "\n",
            "Epoch 7/99\n",
            "----------\n",
            "train Loss: 0.7238 Acc: 0.7310\n",
            "val Loss: 0.5296 Acc: 0.8015\n",
            "\n",
            "Epoch 8/99\n",
            "----------\n",
            "train Loss: 0.6294 Acc: 0.7673\n",
            "val Loss: 0.4745 Acc: 0.8223\n",
            "\n",
            "Epoch 9/99\n",
            "----------\n",
            "train Loss: 0.5963 Acc: 0.7744\n",
            "val Loss: 0.4391 Acc: 0.8390\n",
            "\n",
            "Epoch 10/99\n",
            "----------\n",
            "train Loss: 0.5570 Acc: 0.7952\n",
            "val Loss: 0.4173 Acc: 0.8402\n",
            "\n",
            "Epoch 11/99\n",
            "----------\n",
            "train Loss: 0.5492 Acc: 0.7963\n",
            "val Loss: 0.3953 Acc: 0.8529\n",
            "\n",
            "Epoch 12/99\n",
            "----------\n",
            "train Loss: 0.5415 Acc: 0.8018\n",
            "val Loss: 0.3762 Acc: 0.8627\n",
            "\n",
            "Epoch 13/99\n",
            "----------\n",
            "train Loss: 0.5148 Acc: 0.8119\n",
            "val Loss: 0.3539 Acc: 0.8713\n",
            "\n",
            "Epoch 14/99\n",
            "----------\n",
            "train Loss: 0.4796 Acc: 0.8261\n",
            "val Loss: 0.3466 Acc: 0.8690\n",
            "\n",
            "Epoch 15/99\n",
            "----------\n",
            "train Loss: 0.4765 Acc: 0.8290\n",
            "val Loss: 0.3392 Acc: 0.8719\n",
            "\n",
            "Epoch 16/99\n",
            "----------\n",
            "train Loss: 0.4866 Acc: 0.8187\n",
            "val Loss: 0.3356 Acc: 0.8759\n",
            "\n",
            "Epoch 17/99\n",
            "----------\n",
            "train Loss: 0.4953 Acc: 0.8224\n",
            "val Loss: 0.3324 Acc: 0.8794\n",
            "\n",
            "Epoch 18/99\n",
            "----------\n",
            "train Loss: 0.4685 Acc: 0.8295\n",
            "val Loss: 0.3291 Acc: 0.8794\n",
            "\n",
            "Epoch 19/99\n",
            "----------\n",
            "train Loss: 0.4747 Acc: 0.8242\n",
            "val Loss: 0.3259 Acc: 0.8788\n",
            "\n",
            "Epoch 20/99\n",
            "----------\n",
            "train Loss: 0.4656 Acc: 0.8258\n",
            "val Loss: 0.3208 Acc: 0.8823\n",
            "\n",
            "Epoch 21/99\n",
            "----------\n",
            "train Loss: 0.4597 Acc: 0.8290\n",
            "val Loss: 0.3213 Acc: 0.8823\n",
            "\n",
            "Epoch 22/99\n",
            "----------\n",
            "train Loss: 0.4545 Acc: 0.8288\n",
            "val Loss: 0.3207 Acc: 0.8806\n",
            "\n",
            "Epoch 23/99\n",
            "----------\n",
            "train Loss: 0.4538 Acc: 0.8325\n",
            "val Loss: 0.3206 Acc: 0.8811\n",
            "\n",
            "Epoch 24/99\n",
            "----------\n",
            "train Loss: 0.4506 Acc: 0.8304\n",
            "val Loss: 0.3210 Acc: 0.8817\n",
            "\n",
            "Epoch 25/99\n",
            "----------\n",
            "train Loss: 0.4568 Acc: 0.8331\n",
            "val Loss: 0.3206 Acc: 0.8817\n",
            "\n",
            "Epoch 26/99\n",
            "----------\n",
            "train Loss: 0.4556 Acc: 0.8265\n",
            "val Loss: 0.3204 Acc: 0.8811\n",
            "\n",
            "Epoch 27/99\n",
            "----------\n",
            "train Loss: 0.4602 Acc: 0.8265\n",
            "val Loss: 0.3196 Acc: 0.8817\n",
            "\n",
            "Epoch 28/99\n",
            "----------\n",
            "train Loss: 0.4475 Acc: 0.8379\n",
            "val Loss: 0.3197 Acc: 0.8817\n",
            "\n",
            "Epoch 29/99\n",
            "----------\n",
            "train Loss: 0.4589 Acc: 0.8350\n",
            "val Loss: 0.3201 Acc: 0.8817\n",
            "\n",
            "Epoch 30/99\n",
            "----------\n",
            "train Loss: 0.4539 Acc: 0.8318\n",
            "val Loss: 0.3196 Acc: 0.8806\n",
            "\n",
            "Epoch 31/99\n",
            "----------\n",
            "train Loss: 0.4781 Acc: 0.8222\n",
            "val Loss: 0.3192 Acc: 0.8817\n",
            "\n",
            "Epoch 32/99\n",
            "----------\n",
            "train Loss: 0.4593 Acc: 0.8327\n",
            "val Loss: 0.3198 Acc: 0.8811\n",
            "\n",
            "Epoch 33/99\n",
            "----------\n",
            "train Loss: 0.4564 Acc: 0.8338\n",
            "val Loss: 0.3195 Acc: 0.8811\n",
            "\n",
            "Epoch 34/99\n",
            "----------\n",
            "train Loss: 0.4676 Acc: 0.8299\n",
            "val Loss: 0.3195 Acc: 0.8823\n",
            "\n",
            "Epoch 35/99\n",
            "----------\n",
            "train Loss: 0.4444 Acc: 0.8359\n",
            "val Loss: 0.3193 Acc: 0.8800\n",
            "\n",
            "Epoch 36/99\n",
            "----------\n",
            "train Loss: 0.4418 Acc: 0.8384\n",
            "val Loss: 0.3194 Acc: 0.8806\n",
            "\n",
            "Epoch 37/99\n",
            "----------\n",
            "train Loss: 0.4689 Acc: 0.8265\n",
            "val Loss: 0.3191 Acc: 0.8817\n",
            "\n",
            "Epoch 38/99\n",
            "----------\n",
            "train Loss: 0.4463 Acc: 0.8384\n",
            "val Loss: 0.3195 Acc: 0.8817\n",
            "\n",
            "Epoch 39/99\n",
            "----------\n",
            "train Loss: 0.4586 Acc: 0.8377\n",
            "val Loss: 0.3197 Acc: 0.8806\n",
            "\n",
            "Epoch 40/99\n",
            "----------\n",
            "train Loss: 0.4517 Acc: 0.8318\n",
            "val Loss: 0.3202 Acc: 0.8817\n",
            "\n",
            "Epoch 41/99\n",
            "----------\n",
            "train Loss: 0.4510 Acc: 0.8274\n",
            "val Loss: 0.3200 Acc: 0.8823\n",
            "\n",
            "Epoch 42/99\n",
            "----------\n",
            "train Loss: 0.4584 Acc: 0.8304\n",
            "val Loss: 0.3200 Acc: 0.8834\n",
            "\n",
            "Epoch 43/99\n",
            "----------\n",
            "train Loss: 0.4404 Acc: 0.8402\n",
            "val Loss: 0.3199 Acc: 0.8823\n",
            "\n",
            "Epoch 44/99\n",
            "----------\n",
            "train Loss: 0.4544 Acc: 0.8331\n",
            "val Loss: 0.3194 Acc: 0.8811\n",
            "\n",
            "Epoch 45/99\n",
            "----------\n",
            "train Loss: 0.4581 Acc: 0.8354\n",
            "val Loss: 0.3193 Acc: 0.8834\n",
            "\n",
            "Epoch 46/99\n",
            "----------\n",
            "train Loss: 0.4542 Acc: 0.8327\n",
            "val Loss: 0.3195 Acc: 0.8817\n",
            "\n",
            "Epoch 47/99\n",
            "----------\n",
            "train Loss: 0.4510 Acc: 0.8331\n",
            "val Loss: 0.3195 Acc: 0.8817\n",
            "\n",
            "Epoch 48/99\n",
            "----------\n",
            "train Loss: 0.4489 Acc: 0.8286\n",
            "val Loss: 0.3189 Acc: 0.8817\n",
            "\n",
            "Epoch 49/99\n",
            "----------\n",
            "train Loss: 0.4495 Acc: 0.8313\n",
            "val Loss: 0.3191 Acc: 0.8811\n",
            "\n",
            "Epoch 50/99\n",
            "----------\n",
            "train Loss: 0.4639 Acc: 0.8320\n",
            "val Loss: 0.3191 Acc: 0.8806\n",
            "\n",
            "Epoch 51/99\n",
            "----------\n",
            "train Loss: 0.4485 Acc: 0.8363\n",
            "val Loss: 0.3195 Acc: 0.8794\n",
            "\n",
            "Epoch 52/99\n",
            "----------\n",
            "train Loss: 0.4584 Acc: 0.8299\n",
            "val Loss: 0.3190 Acc: 0.8817\n",
            "\n",
            "Epoch 53/99\n",
            "----------\n",
            "train Loss: 0.4603 Acc: 0.8341\n",
            "val Loss: 0.3195 Acc: 0.8823\n",
            "\n",
            "Epoch 54/99\n",
            "----------\n",
            "train Loss: 0.4823 Acc: 0.8190\n",
            "val Loss: 0.3193 Acc: 0.8834\n",
            "\n",
            "Epoch 55/99\n",
            "----------\n",
            "train Loss: 0.4605 Acc: 0.8315\n",
            "val Loss: 0.3189 Acc: 0.8817\n",
            "\n",
            "Epoch 56/99\n",
            "----------\n",
            "train Loss: 0.4780 Acc: 0.8201\n",
            "val Loss: 0.3191 Acc: 0.8823\n",
            "\n",
            "Epoch 57/99\n",
            "----------\n",
            "train Loss: 0.4623 Acc: 0.8281\n",
            "val Loss: 0.3197 Acc: 0.8817\n",
            "\n",
            "Epoch 58/99\n",
            "----------\n",
            "train Loss: 0.4745 Acc: 0.8265\n",
            "val Loss: 0.3198 Acc: 0.8817\n",
            "\n",
            "Epoch 59/99\n",
            "----------\n",
            "train Loss: 0.4498 Acc: 0.8354\n",
            "val Loss: 0.3199 Acc: 0.8794\n",
            "\n",
            "Epoch 60/99\n",
            "----------\n",
            "train Loss: 0.4576 Acc: 0.8359\n",
            "val Loss: 0.3201 Acc: 0.8800\n",
            "\n",
            "Epoch 61/99\n",
            "----------\n",
            "train Loss: 0.4555 Acc: 0.8267\n",
            "val Loss: 0.3203 Acc: 0.8817\n",
            "\n",
            "Epoch 62/99\n",
            "----------\n",
            "train Loss: 0.4543 Acc: 0.8354\n",
            "val Loss: 0.3198 Acc: 0.8800\n",
            "\n",
            "Epoch 63/99\n",
            "----------\n",
            "train Loss: 0.4399 Acc: 0.8455\n",
            "val Loss: 0.3196 Acc: 0.8829\n",
            "\n",
            "Epoch 64/99\n",
            "----------\n",
            "train Loss: 0.4637 Acc: 0.8299\n",
            "val Loss: 0.3193 Acc: 0.8829\n",
            "\n",
            "Epoch 65/99\n",
            "----------\n",
            "train Loss: 0.4493 Acc: 0.8361\n",
            "val Loss: 0.3197 Acc: 0.8823\n",
            "\n",
            "Epoch 66/99\n",
            "----------\n",
            "train Loss: 0.4713 Acc: 0.8304\n",
            "val Loss: 0.3192 Acc: 0.8811\n",
            "\n",
            "Epoch 67/99\n",
            "----------\n",
            "train Loss: 0.4556 Acc: 0.8329\n",
            "val Loss: 0.3192 Acc: 0.8823\n",
            "\n",
            "Epoch 68/99\n",
            "----------\n",
            "train Loss: 0.4422 Acc: 0.8389\n",
            "val Loss: 0.3188 Acc: 0.8829\n",
            "\n",
            "Epoch 69/99\n",
            "----------\n",
            "train Loss: 0.4609 Acc: 0.8320\n",
            "val Loss: 0.3190 Acc: 0.8806\n",
            "\n",
            "Epoch 70/99\n",
            "----------\n",
            "train Loss: 0.4465 Acc: 0.8416\n",
            "val Loss: 0.3189 Acc: 0.8823\n",
            "\n",
            "Epoch 71/99\n",
            "----------\n",
            "train Loss: 0.4739 Acc: 0.8272\n",
            "val Loss: 0.3196 Acc: 0.8806\n",
            "\n",
            "Epoch 72/99\n",
            "----------\n",
            "train Loss: 0.4567 Acc: 0.8299\n",
            "val Loss: 0.3199 Acc: 0.8806\n",
            "\n",
            "Epoch 73/99\n",
            "----------\n",
            "train Loss: 0.4616 Acc: 0.8297\n",
            "val Loss: 0.3194 Acc: 0.8817\n",
            "\n",
            "Epoch 74/99\n",
            "----------\n",
            "train Loss: 0.4440 Acc: 0.8352\n",
            "val Loss: 0.3194 Acc: 0.8823\n",
            "\n",
            "Epoch 75/99\n",
            "----------\n",
            "train Loss: 0.4548 Acc: 0.8290\n",
            "val Loss: 0.3197 Acc: 0.8806\n",
            "\n",
            "Epoch 76/99\n",
            "----------\n",
            "train Loss: 0.4575 Acc: 0.8350\n",
            "val Loss: 0.3197 Acc: 0.8811\n",
            "\n",
            "Epoch 77/99\n",
            "----------\n",
            "train Loss: 0.4629 Acc: 0.8295\n",
            "val Loss: 0.3200 Acc: 0.8800\n",
            "\n",
            "Epoch 78/99\n",
            "----------\n",
            "train Loss: 0.4647 Acc: 0.8267\n",
            "val Loss: 0.3198 Acc: 0.8817\n",
            "\n",
            "Epoch 79/99\n",
            "----------\n",
            "train Loss: 0.4564 Acc: 0.8338\n",
            "val Loss: 0.3196 Acc: 0.8823\n",
            "\n",
            "Epoch 80/99\n",
            "----------\n",
            "train Loss: 0.4623 Acc: 0.8338\n",
            "val Loss: 0.3191 Acc: 0.8811\n",
            "\n",
            "Epoch 81/99\n",
            "----------\n",
            "train Loss: 0.4496 Acc: 0.8352\n",
            "val Loss: 0.3191 Acc: 0.8806\n",
            "\n",
            "Epoch 82/99\n",
            "----------\n",
            "train Loss: 0.4489 Acc: 0.8377\n",
            "val Loss: 0.3188 Acc: 0.8806\n",
            "\n",
            "Epoch 83/99\n",
            "----------\n",
            "train Loss: 0.4575 Acc: 0.8329\n",
            "val Loss: 0.3189 Acc: 0.8800\n",
            "\n",
            "Epoch 84/99\n",
            "----------\n",
            "train Loss: 0.4650 Acc: 0.8311\n",
            "val Loss: 0.3190 Acc: 0.8829\n",
            "\n",
            "Epoch 85/99\n",
            "----------\n",
            "train Loss: 0.4671 Acc: 0.8341\n",
            "val Loss: 0.3194 Acc: 0.8806\n",
            "\n",
            "Epoch 86/99\n",
            "----------\n",
            "train Loss: 0.4587 Acc: 0.8261\n",
            "val Loss: 0.3200 Acc: 0.8817\n",
            "\n",
            "Epoch 87/99\n",
            "----------\n",
            "train Loss: 0.4538 Acc: 0.8281\n",
            "val Loss: 0.3193 Acc: 0.8811\n",
            "\n",
            "Epoch 88/99\n",
            "----------\n",
            "train Loss: 0.4787 Acc: 0.8210\n",
            "val Loss: 0.3192 Acc: 0.8806\n",
            "\n",
            "Epoch 89/99\n",
            "----------\n",
            "train Loss: 0.4646 Acc: 0.8293\n",
            "val Loss: 0.3194 Acc: 0.8817\n",
            "\n",
            "Epoch 90/99\n",
            "----------\n",
            "train Loss: 0.4840 Acc: 0.8201\n",
            "val Loss: 0.3196 Acc: 0.8811\n",
            "\n",
            "Epoch 91/99\n",
            "----------\n",
            "train Loss: 0.4673 Acc: 0.8245\n",
            "val Loss: 0.3196 Acc: 0.8806\n",
            "\n",
            "Epoch 92/99\n",
            "----------\n",
            "train Loss: 0.4387 Acc: 0.8375\n",
            "val Loss: 0.3197 Acc: 0.8817\n",
            "\n",
            "Epoch 93/99\n",
            "----------\n",
            "train Loss: 0.4546 Acc: 0.8311\n",
            "val Loss: 0.3196 Acc: 0.8817\n",
            "\n",
            "Epoch 94/99\n",
            "----------\n",
            "train Loss: 0.4587 Acc: 0.8279\n",
            "val Loss: 0.3194 Acc: 0.8823\n",
            "\n",
            "Epoch 95/99\n",
            "----------\n",
            "train Loss: 0.4610 Acc: 0.8359\n",
            "val Loss: 0.3196 Acc: 0.8811\n",
            "\n",
            "Epoch 96/99\n",
            "----------\n",
            "train Loss: 0.4507 Acc: 0.8297\n",
            "val Loss: 0.3193 Acc: 0.8823\n",
            "\n",
            "Epoch 97/99\n",
            "----------\n",
            "train Loss: 0.4527 Acc: 0.8336\n",
            "val Loss: 0.3192 Acc: 0.8840\n",
            "\n",
            "Epoch 98/99\n",
            "----------\n",
            "train Loss: 0.4715 Acc: 0.8270\n",
            "val Loss: 0.3197 Acc: 0.8817\n",
            "\n",
            "Epoch 99/99\n",
            "----------\n",
            "train Loss: 0.4530 Acc: 0.8290\n",
            "val Loss: 0.3195 Acc: 0.8811\n",
            "\n",
            "Training complete in 35m 52s\n",
            "Best val Acc: 0.884016\n"
          ]
        }
      ],
      "source": [
        "model_metadata = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyEWH4SmfjIs"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-hCtMg7fjIt"
      },
      "outputs": [],
      "source": [
        "train_acc, train_loss = [], []\n",
        "val_acc, val_loss = [], []\n",
        "for i, (acc, loss) in enumerate(zip(model_metadata['epoch_accuracy'], model_metadata['epoch_loss'])):\n",
        "    if i % 2 == 0:\n",
        "        train_acc.append(acc)\n",
        "        train_loss.append(loss)\n",
        "    else:\n",
        "        val_acc.append(acc)\n",
        "        val_loss.append(loss)"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "D4nkzBLtfjIu"
      },
      "source": [
        "Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WA9TWjmfjIv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Tc95_zKfjIv"
      },
      "outputs": [],
      "source": [
        "val_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "val_dataset = datasets.ImageFolder('/home/mega/Desktop/Data2/Datasets/Thyroid_Dataset/pytorch_dataset_duc/pytorch_dataset_duc/val/', val_transform)\n",
        "dataloaders = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "dataset_sizes = len(val_dataset)\n",
        "class_names = val_dataset.classes\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "running_loss = 0.0\n",
        "running_corrects = 0\n",
        "sm_probability = []\n",
        "pred_labels = []\n",
        "true_labels = []\n",
        "prob_crit = nn.CrossEntropyLoss()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, (inputs, labels) in enumerate(dataloaders):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        #labels = labels - 1\n",
        "        outputs = model_metadata['model'](inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, labels)\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "        loss = prob_crit(outputs, labels)\n",
        "        \n",
        "        p = torch.nn.functional.softmax(outputs, dim=1)\n",
        "        numpy_p = p.cpu().numpy().tolist()\n",
        "        numpy_labels = labels.cpu().numpy().tolist()\n",
        "        \n",
        "        sm_probability.extend(numpy_p)\n",
        "        true_labels.extend(numpy_labels)\n",
        "        pred_labels.extend(preds.cpu().numpy().tolist())\n",
        "    sm_probability = np.array(sm_probability)\n",
        "    true_labels = np.array(true_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktH3RzS5fjIv",
        "outputId": "f86563b4-ffd0-410b-c6e5-2a703fa75657"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1.09029454e-04, 6.52553886e-03, 9.91666734e-01, 7.01401950e-05,\n",
              "        1.62846304e-03],\n",
              "       [1.30784574e-05, 2.70844903e-05, 9.99870181e-01, 7.00580713e-05,\n",
              "        1.96166566e-05],\n",
              "       [6.41948136e-05, 3.43087188e-04, 1.00151624e-03, 1.67699509e-05,\n",
              "        9.98574495e-01],\n",
              "       ...,\n",
              "       [9.83837545e-01, 6.26693526e-03, 2.26609455e-03, 5.56135178e-03,\n",
              "        2.06804811e-03],\n",
              "       [2.05606939e-05, 1.87769692e-04, 1.90358653e-04, 6.44202065e-03,\n",
              "        9.93159235e-01],\n",
              "       [2.37882161e-03, 3.52490813e-01, 9.70513225e-02, 5.34946561e-01,\n",
              "        1.31324157e-02]])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "true_labels\n",
        "sm_probability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5womY248fjIw",
        "outputId": "3945d6f5-c1b1-421c-b25f-baa01fe33abd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 0.31924698700880083 - Acc: 0.8840161569532602\n"
          ]
        }
      ],
      "source": [
        "val_loss = running_loss / dataset_sizes\n",
        "val_acc = running_corrects.double() / dataset_sizes\n",
        "val_loss, val_acc\n",
        "print(f'Loss: {val_loss} - Acc: {val_acc}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHtGGGp9fjIw",
        "outputId": "9ce4dfcd-a676-4e81-8a90-4e5d42646ffd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion matrix\n",
            " [[264  14  10  10   1]\n",
            " [ 11 237   7  11   9]\n",
            " [  8   5 420  11  13]\n",
            " [  9  12   6 259  17]\n",
            " [  6  11  10  20 352]] \n",
            "---------------------------------\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mega/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
            "  app.launch_new_instance()\n",
            "/home/mega/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: FixedFormatter should only be used together with FixedLocator\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.88      0.88       299\n",
            "           1       0.85      0.86      0.86       275\n",
            "           2       0.93      0.92      0.92       457\n",
            "           3       0.83      0.85      0.84       303\n",
            "           4       0.90      0.88      0.89       399\n",
            "\n",
            "    accuracy                           0.88      1733\n",
            "   macro avg       0.88      0.88      0.88      1733\n",
            "weighted avg       0.88      0.88      0.88      1733\n",
            "\n",
            "Confusion matrix\n",
            " [[264  14  10  10   1]\n",
            " [ 11 237   7  11   9]\n",
            " [  8   5 420  11  13]\n",
            " [  9  12   6 259  17]\n",
            " [  6  11  10  20 352]] \n",
            "---------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from ipykernel import kernelapp as app\n",
        "#app.launch_new_instance()\n",
        "y_true = true_labels\n",
        "y_pred = pred_labels\n",
        "labels_cat = ['Cat2', 'Cat3' , 'Cat4' , 'Cat5' , 'Cat6']\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "print('Confusion matrix\\n', cm, '\\n---------------------------------\\n')\n",
        "\n",
        "\n",
        "fig = plt.figure() \n",
        "ax = fig.add_subplot(111) \n",
        "cax = ax.matshow(cm) \n",
        "#plt.title('Confusion matrix of the classifier') \n",
        "fig.colorbar(cax) \n",
        "ax.set_xticklabels([''] + labels_cat) \n",
        "ax.set_yticklabels([''] + labels_cat) \n",
        "plt.xlabel('Predicted Category') \n",
        "plt.ylabel('True Category') \n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "#plt.matshow(cm)\n",
        "#plt.colorbar()\n",
        "#plt.show()\n",
        "\n",
        "print(classification_report(y_true, y_pred))\n",
        "print('Confusion matrix\\n', confusion_matrix(y_true, y_pred), '\\n---------------------------------\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTm2NDQ1fjIx",
        "outputId": "01314559-626d-4836-d56c-456208b02a0d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([264, 237, 420, 259, 352])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cm=confusion_matrix(y_true,y_pred)\n",
        "tp=np.diag(cm)\n",
        "tp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qs4f1FPAfjIx",
        "outputId": "14f7fe32-6688-4f6d-c63e-59755338677e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([34, 42, 33, 52, 40])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fp = np.sum(cm, axis=0) - tp\n",
        "fp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMM8BIV5fjIy",
        "outputId": "4791f481-5e05-49a6-aae2-02d068b80939"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([35, 38, 37, 44, 47])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fn = np.sum(cm, axis=1) - tp\n",
        "fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQBPg6X_fjIy",
        "outputId": "709a0e00-01a0-47b7-cd14-1567207f20af"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1400, 1416, 1243, 1378, 1294]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_classes = 5\n",
        "tn = []\n",
        "for i in range(num_classes):\n",
        "    temp = np.delete(cm, i, 0)    # delete ith row\n",
        "    temp = np.delete(temp, i, 1)  # delete ith column\n",
        "    tn.append(sum(sum(temp)))\n",
        "tn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__8hAdICfjIz"
      },
      "outputs": [],
      "source": [
        "# tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "# tn, fp, fn, tp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2CItJaPfjIz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLRylJJLfjIz",
        "outputId": "b7dd3e5c-2ae7-44c3-8c28-d26613137ac3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sensitivity: [0.88294314 0.86181818 0.9190372  0.85478548 0.88220551] - Specificity: [0.9762901  0.97119342 0.97413793 0.96363636 0.97001499]\n"
          ]
        }
      ],
      "source": [
        "# REF: \n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
        "# https://towardsdatascience.com/evaluating-categorical-models-ii-sensitivity-and-specificity-e181e573cff8\n",
        "\n",
        "# recall of the positive class is also known as sensitivity: is the metric that evaluates a models ability to predict true positives of each available category\n",
        "sensitivity = tp / (tp + fn)\n",
        "#recall of the negative class is specificity:  the metric that evaluates a models ability to predict true negatives of each available category\n",
        "specificity = tn / (tn + fp)\n",
        "print(f'Sensitivity: {sensitivity} - Specificity: {specificity}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyk4I5EafjI0",
        "outputId": "1a1f65af-de61-4904-ca9a-01b6ccf0103b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PPV: [0.88590604 0.84946237 0.92715232 0.83279743 0.89795918] - NPV: [0.97560976 0.9738652  0.97109375 0.96905767 0.96495153]\n"
          ]
        }
      ],
      "source": [
        "# PPV, NPV \n",
        "# REF: https://en.wikipedia.org/wiki/Positive_and_negative_predictive_values\n",
        "ppv = tp / (tp + fp)\n",
        "npv = tn / (tn + fn)\n",
        "print(f'PPV: {ppv} - NPV: {npv}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6FFt_vpfjI0"
      },
      "outputs": [],
      "source": [
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "test_dataset = datasets.ImageFolder('/home/mega/Desktop/Data2/Datasets/Thyroid_Dataset/pytorch_dataset_duc/pytorch_dataset_duc/test/', test_transform)\n",
        "dataloaders = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
        "dataset_sizes = len(test_dataset)\n",
        "class_names = test_dataset.classes\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "running_loss = 0.0\n",
        "running_corrects = 0\n",
        "sm_probability = []\n",
        "pred_labels = []\n",
        "true_labels = []\n",
        "prob_crit = nn.CrossEntropyLoss()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, (inputs, labels) in enumerate(dataloaders):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        #labels = labels - 1\n",
        "        outputs = model_metadata['model'](inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, labels)\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "        loss = prob_crit(outputs, labels)\n",
        "        p = torch.nn.functional.softmax(outputs, dim=1)\n",
        "        numpy_p = p.cpu().numpy().tolist()\n",
        "        numpy_labels = labels.cpu().numpy().tolist()\n",
        "        sm_probability.extend(numpy_p)\n",
        "        true_labels.extend(numpy_labels)\n",
        "        pred_labels.extend(preds.cpu().numpy().tolist())\n",
        "    sm_probability = np.array(sm_probability)\n",
        "    true_labels = np.array(true_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJCLg89rfjI0",
        "outputId": "bc802447-2e1b-42e4-ddb2-7b680cf5dc3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 0.3063960498831191 - Acc: 0.8971553610503282\n"
          ]
        }
      ],
      "source": [
        "test_loss = running_loss / dataset_sizes\n",
        "test_acc = running_corrects.double() / dataset_sizes\n",
        "test_loss, test_acc\n",
        "print(f'Loss: {test_loss} - Acc: {test_acc}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMve9ddcfjI1",
        "outputId": "da6bcf67-7250-4fdc-be0f-c26b2e80e07f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion matrix\n",
            " [[156   8   5   6   1]\n",
            " [  5 107   6   5   2]\n",
            " [  5   2 239   3   2]\n",
            " [  6   6   3 120   5]\n",
            " [  5   4   3  12 198]] \n",
            "---------------------------------\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mega/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
            "  \n",
            "/home/mega/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.89      0.88       176\n",
            "           1       0.84      0.86      0.85       125\n",
            "           2       0.93      0.95      0.94       251\n",
            "           3       0.82      0.86      0.84       140\n",
            "           4       0.95      0.89      0.92       222\n",
            "\n",
            "    accuracy                           0.90       914\n",
            "   macro avg       0.89      0.89      0.89       914\n",
            "weighted avg       0.90      0.90      0.90       914\n",
            "\n",
            "Confusion matrix\n",
            " [[156   8   5   6   1]\n",
            " [  5 107   6   5   2]\n",
            " [  5   2 239   3   2]\n",
            " [  6   6   3 120   5]\n",
            " [  5   4   3  12 198]] \n",
            "---------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "y_true = true_labels\n",
        "y_pred = pred_labels\n",
        "labels_cat = ['Cat2', 'Cat3' , 'Cat4' , 'Cat5' , 'Cat6']\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "print('Confusion matrix\\n', cm, '\\n---------------------------------\\n')\n",
        "\n",
        "\n",
        "fig = plt.figure() \n",
        "ax = fig.add_subplot(111) \n",
        "cax = ax.matshow(cm) \n",
        "#plt.title('Confusion matrix of the classifier') \n",
        "fig.colorbar(cax) \n",
        "ax.set_xticklabels([''] + labels_cat) \n",
        "ax.set_yticklabels([''] + labels_cat) \n",
        "plt.xlabel('Predicted Category') \n",
        "plt.ylabel('True Category') \n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "#plt.matshow(cm)\n",
        "#plt.colorbar()\n",
        "#plt.show()\n",
        "\n",
        "print(classification_report(y_true, y_pred))\n",
        "print('Confusion matrix\\n', confusion_matrix(y_true, y_pred), '\\n---------------------------------\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ol8_8VEYfjI1"
      },
      "outputs": [],
      "source": [
        "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "tn, fp, fn, tp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkkxsN9_fjI1",
        "outputId": "d53d326d-db9f-4c0a-e3a7-c063aed9b381"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([156, 107, 239, 120, 198])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cm=confusion_matrix(y_true,y_pred)\n",
        "tp=np.diag(cm)\n",
        "tp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOMSNkXhfjI2",
        "outputId": "3a1bef5f-6419-4718-90dc-45aba28c983f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([21, 20, 17, 26, 10])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fp = np.sum(cm, axis=0) - tp\n",
        "fp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HZcRwlKfjI2",
        "outputId": "08645ae4-c601-4494-f003-2cd5333f2ede"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([20, 18, 12, 20, 24])"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fn = np.sum(cm, axis=1) - tp\n",
        "fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-mZlTX4fjI2",
        "outputId": "d07bd254-1253-41e9-cae7-cb9cd397a5aa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[717, 769, 646, 748, 682]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_classes = 5\n",
        "tn = []\n",
        "for i in range(num_classes):\n",
        "    temp = np.delete(cm, i, 0)    # delete ith row\n",
        "    temp = np.delete(temp, i, 1)  # delete ith column\n",
        "    tn.append(sum(sum(temp)))\n",
        "tn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09FrLeRNfjI3",
        "outputId": "0fa65026-a571-4242-be3c-3e55569bf7bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sensitivity: [0.88636364 0.856      0.95219124 0.85714286 0.89189189] - Specificity: [0.97154472 0.97465146 0.97435897 0.96640827 0.98554913]\n"
          ]
        }
      ],
      "source": [
        "# REF: \n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
        "# https://towardsdatascience.com/evaluating-categorical-models-ii-sensitivity-and-specificity-e181e573cff8\n",
        "\n",
        "# recall of the positive class is also known as sensitivity: is the metric that evaluates a models ability to predict true positives of each available category\n",
        "sensitivity = tp / (tp + fn)\n",
        "#recall of the negative class is specificity:  the metric that evaluates a models ability to predict true negatives of each available category\n",
        "specificity = tn / (tn + fp)\n",
        "print(f'Sensitivity: {sensitivity} - Specificity: {specificity}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvxAL8_kfjI3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gy3gU4JpfjI3",
        "outputId": "3e1b67a3-6431-484e-c521-b93e1bce59b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PPV: [0.88135593 0.84251969 0.93359375 0.82191781 0.95192308] - NPV: [0.97286296 0.97712834 0.98176292 0.97395833 0.96600567]\n"
          ]
        }
      ],
      "source": [
        "# PPV, NPV \n",
        "# REF: https://en.wikipedia.org/wiki/Positive_and_negative_predictive_values\n",
        "ppv = tp / (tp + fp)\n",
        "npv = tn / (tn + fn)\n",
        "print(f'PPV: {ppv} - NPV: {npv}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ICMYjz0fjI4"
      },
      "outputs": [],
      "source": [
        "torch.save(model_metadata['model'], '/home/mega/Desktop/Data2/Datasets/Thyroid_Dataset/Efficient_bo_Two_Atten_Parallel.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOrJZgj_fjI4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py36",
      "language": "python",
      "name": "py36"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}